{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Train_BiLSTM_with_PyTorch_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxluDUWtTy3f"
      },
      "source": [
        "## Import needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CWij87vx2-9",
        "outputId": "3b82b47d-0ecd-4f7b-c3d9-24be1c154617"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb7fc18aad0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT5ts8PtVmYo"
      },
      "source": [
        "## Prepare data and create mapping dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZirOttACuJ8k"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    ('I work at The University of British Columbia'.split(), ['O','O','O','B-org','I-org','I-org','I-org','I-org']),\n",
        "    ('Vancouver is a beautiful city'.split(), ['B-geo', 'O', 'O', 'O','O']),\n",
        "    ('Apple is about to unveil the newest device'.split(), ['B-org','O','O','O','O','O','O','O'])\n",
        "]\n",
        "\n",
        "# get word-to-id and tag-to-id dictionary\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  \n",
        "            word_to_ix[word] = len(word_to_ix)  \n",
        "\n",
        "tag_to_ix = {'O':0, 'B-org':1, 'I-org':2, 'B-geo':3}\n",
        "\n",
        "# get id-to-word and id-to-tag dictionary\n",
        "ix_to_word = {ix:word for word, ix in word_to_ix.items()}\n",
        "ix_to_tag = {ix:tag for tag, ix in tag_to_ix.items()}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23m6V5TEV0YQ"
      },
      "source": [
        "## Define BiLSTM model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJfyzSIIuTfY"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYhS-DyV7sH"
      },
      "source": [
        "## Define evaluation function for precison, recall, accuracy, F score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixSYXTq8xP4-"
      },
      "source": [
        "def evaluate(predictions: List[List[int]], ground_truths: List[List[int]]) -> tuple: #NER precision, recall have many different definitions, refer to https://github.com/MantisAI/nervaluate\n",
        "  # TP: this token is a named entity of type X and it is predicted X\n",
        "  # FP: this token is not a named entity but is predicted to be one\n",
        "  # TN: this token is not a named entity and it is predicted as O\n",
        "  # FN: this token is a name entity but is not predicted to be one\n",
        "  TP, FP, TN, FN = 0, 0, 0, 0\n",
        "  named_entity_tags = ix_to_tag.keys() - {0} # all named entity tags but 0 which represents the id of non-named-entity token\n",
        "  for i in range(len(predictions)):\n",
        "    prediction = predictions[i]\n",
        "    for j in range(len(prediction)):\n",
        "      token_tag_pred = prediction[j]\n",
        "      token_tag_ground_truth = ground_truths[i][j]\n",
        "\n",
        "      if (token_tag_pred in named_entity_tags):\n",
        "        if token_tag_pred == token_tag_ground_truth: # TP\n",
        "          TP += 1\n",
        "        else:\n",
        "          FP += 1\n",
        "      else: #not predicted as named entity\n",
        "        if token_tag_pred == token_tag_ground_truth: # TP\n",
        "          TN += 1\n",
        "        else:\n",
        "          FN += 1\n",
        "  # precision = TP / (TP + FP) if (TP + FP) != 0 else 1 #avoid devide by zero\n",
        "  # recall = TP / (TP + FN) if (TP + FN) != 0 else 1 #avoid devide by zero\n",
        "  acc = (TP + TN) / (TP + FP + TN + FN)\n",
        "  # f_score = 2 * (recall * precision) / (recall + precision) if (recall + precision) != 0 else 0 #avoid devide by zero\n",
        "  # return precision, recall, acc, f_score\n",
        "  return acc"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0R2wQAfWG2Z"
      },
      "source": [
        "## Start training, compare metrics bofore and after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA17CYdzuUgH",
        "outputId": "c8580926-eceb-4b04-e9c6-9360d0c7c88f"
      },
      "source": [
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 100\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "Y = [[tag_to_ix[tag] for tag in y] for x,y in training_data]\n",
        "\n",
        "# doing inference before training\n",
        "with torch.no_grad():\n",
        "    Y_hat = []\n",
        "    for sentence, tags in training_data:\n",
        "      inputs = prepare_sequence(sentence, word_to_ix)\n",
        "      # (log) prob distribution of tags for each token\n",
        "      tag_scores = model(inputs) # a matrix, e.g. [[-5.7220e+00, -9.6667e-02, -4.2361e+00, -3.5866e+00, -3.6130e+00, -3.9250e+00],[-7.2715e+00, -2.7125e-02, -5.5188e+00, -4.0830e+00, -5.5646e+00,-6.5950e+00],...], each sublist is a (log) distribution of each token\n",
        "      # argmax to extract the index of highest-prob tag; and convert tensor back to list for further procedure (there is no map() for tensor to convert id back to tag and apply_() is in-place operation which requires converted dtype to be identical to what tensor is declared)\n",
        "      pred = torch.argmax(tag_scores, dim=1).tolist() \n",
        "      Y_hat.append(pred)\n",
        "\n",
        "# precision, recall, acc, f_score = evaluate(Y_hat, Y)\n",
        "# print('Before training:', {'precision':precision, 'recall':recall, 'acc':acc, 'f_score': f_score})\n",
        "before_training_acc = evaluate(Y_hat, Y)\n",
        "print('Before training acc:', before_training_acc)\n",
        "\n",
        "for epoch in range(300): \n",
        "    for sentence, tags in training_data:\n",
        "        model.zero_grad()\n",
        "\n",
        "        # convert word/tag to ids and convert list to tensor\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # forward pass\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# doing inference after training\n",
        "with torch.no_grad():\n",
        "    Y_hat = []\n",
        "    for sentence, tags in training_data:\n",
        "      inputs = prepare_sequence(sentence, word_to_ix)\n",
        "      # (log) prob distribution of tags for each token\n",
        "      tag_scores = model(inputs) # a matrix, e.g. [[-5.7220e+00, -9.6667e-02, -4.2361e+00, -3.5866e+00, -3.6130e+00, -3.9250e+00],[-7.2715e+00, -2.7125e-02, -5.5188e+00, -4.0830e+00, -5.5646e+00,-6.5950e+00],...], each sublist is a (log) distribution of each token\n",
        "      # argmax to extract the index of highest-prob tag; and convert tensor back to list for further procedure (there is no map() for tensor to convert id back to tag and apply_() is in-place operation which requires converted dtype to be identical to what tensor is declared)\n",
        "      pred = torch.argmax(tag_scores, dim=1).tolist() \n",
        "      Y_hat.append(pred)\n",
        "\n",
        "# precision, recall, acc, f_score = evaluate(Y_hat, Y)\n",
        "# print('After training:', {'precision':precision, 'recall':recall, 'acc':acc, 'f_score': f_score})\n",
        "after_training_acc = evaluate(Y_hat, Y)\n",
        "print('After training acc:', after_training_acc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training acc: 0.14285714285714285\n",
            "After training acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3EghVwDDOsK"
      },
      "source": [
        "## References: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    }
  ]
}